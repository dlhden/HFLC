{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edbc38e-9e43-4429-8277-0efea25efe51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:03.485705Z",
     "iopub.status.busy": "2023-06-01T02:51:03.485074Z",
     "iopub.status.idle": "2023-06-01T02:51:06.008280Z",
     "shell.execute_reply": "2023-06-01T02:51:06.007164Z",
     "shell.execute_reply.started": "2023-06-01T02:51:03.485662Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import MutableMapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Iterable, Mapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Sized\r\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.static import InputSpec\n",
    "from paddle.fluid.framework import core\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import functools\n",
    "\n",
    "import gc\n",
    "from itertools import repeat\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "# from saliency.smooth_grad import SmoothGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d91662f-425e-4776-aae3-a927bb0ccef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:06.011352Z",
     "iopub.status.busy": "2023-06-01T02:51:06.010802Z",
     "iopub.status.idle": "2023-06-01T02:51:06.015988Z",
     "shell.execute_reply": "2023-06-01T02:51:06.015025Z",
     "shell.execute_reply.started": "2023-06-01T02:51:06.011323Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learning_rate = 1e-3\n",
    "learning_rate = 2e-4\n",
    "# learning_rate = 3e-1\n",
    "\n",
    "batch_size = 128\n",
    "# batch_size = 32\n",
    "\n",
    "samples = 50000\n",
    "test_samples = 10000\n",
    "examples = samples\n",
    "n_class = 100\n",
    "\n",
    "training_epochs = 180\n",
    "\n",
    "noise_rate = 0.8\n",
    "\n",
    "image_size = 112\n",
    "\n",
    "t_w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa88141-052e-4e83-bb0c-292626011119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:06.017191Z",
     "iopub.status.busy": "2023-06-01T02:51:06.016960Z",
     "iopub.status.idle": "2023-06-01T02:51:06.139328Z",
     "shell.execute_reply": "2023-06-01T02:51:06.138351Z",
     "shell.execute_reply.started": "2023-06-01T02:51:06.017170Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(paddle.io.Dataset):\n",
    "    def __init__(self, num_samples, file_path, save_path, transform, mode = 'train',):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.dataset = paddle.vision.datasets.Cifar100(mode=mode, transform=self.transform, \n",
    "        download = False, data_file = file_path)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            self.labels = np.load(save_path)\n",
    "        else:\n",
    "            self.labels = []\n",
    "            for d in tqdm.tqdm(self.dataset):\n",
    "                self.labels.append(label_to_truelabel[str(d[1])])\n",
    "            self.labels = np.array(self.labels).reshape(-1,)\n",
    "            np.save(save_path, self.labels)\n",
    "         \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index][0]\n",
    "        # data = np.array(data)\n",
    "        \n",
    "        # if self.transform is not None:\n",
    "        #     data = self.transform(data)\n",
    "        return index, data\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1de0ee-158c-4472-b057-5a092f5fc394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:06.141099Z",
     "iopub.status.busy": "2023-06-01T02:51:06.140679Z",
     "iopub.status.idle": "2023-06-01T02:51:11.981481Z",
     "shell.execute_reply": "2023-06-01T02:51:11.980352Z",
     "shell.execute_reply.started": "2023-06-01T02:51:06.141071Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download training data and load training data.\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\n",
    "from paddle.vision.transforms import functional as F\n",
    "from paddle.vision.transforms import BaseTransform\n",
    "\n",
    "\n",
    "mean,std = ([0.4914, 0.4822, 0.4465],[0.2471, 0.2435,0.2616])\n",
    "mean = np.array(mean).reshape(1,1,3)*255\n",
    "std = np.array(std).reshape(1,1,3)*255\n",
    "\n",
    "transform_robust = T.Compose([\n",
    "                    T.Resize(image_size),\n",
    "                    T.Transpose(order = (2,1,0)),\n",
    "                    T.RandomHorizontalFlip(0.5),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "transform_clean = T.Compose([\n",
    "                    T.Resize(image_size),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "transform_show = T.Compose([\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    ])\n",
    "transform_new = T.Compose([\n",
    "                    T.RandomHorizontalFlip(0.5),\n",
    "                    T.RandomRotation(10),\n",
    "                    T.Resize((image_size, image_size)),\n",
    "                    T.Transpose(order=(2,1,0,)),\n",
    "                    T.Normalize(mean=mean,std=std)\n",
    "                    ])\n",
    "\n",
    "print('Start download training data and load training data.')\n",
    "train_dataset = MyDataset(samples, r'/home/aistudio/data/data76994/cifar-100-python.tar.gz',\n",
    "r'./work/cifar-100-train_Y.npy',transform_robust, mode = 'train')\n",
    "\n",
    "test_dataset = MyDataset(test_samples, r'/home/aistudio/data/data76994/cifar-100-python.tar.gz',\n",
    " r'./work/cifar-100-test_Y.npy',transform_clean, mode = 'test')\n",
    "\n",
    "# d = paddle.vision.datasets.Cifar100(mode='train', transform=transform_robust, \n",
    "#         download = False, data_file = r'/home/aistudio/data/data76994/cifar-100-python.tar.gz')\n",
    "print('Finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f5497e-3020-428b-a7a6-bcb701f68d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:11.983062Z",
     "iopub.status.busy": "2023-06-01T02:51:11.982775Z",
     "iopub.status.idle": "2023-06-01T02:51:11.987841Z",
     "shell.execute_reply": "2023-06-01T02:51:11.987017Z",
     "shell.execute_reply.started": "2023-06-01T02:51:11.983035Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6,)\n",
    "valid_loader = paddle.io.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc99fb8-48ea-4f44-9855-2bd66e4db0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:11.989323Z",
     "iopub.status.busy": "2023-06-01T02:51:11.988965Z",
     "iopub.status.idle": "2023-06-01T02:51:11.997375Z",
     "shell.execute_reply": "2023-06-01T02:51:11.996589Z",
     "shell.execute_reply.started": "2023-06-01T02:51:11.989298Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(r'work/label_to_truelabel.json') as f:\n",
    "    label_to_truelabel = json.load(f)\n",
    "\n",
    "with open(r'work/label_to_w.json') as f:\n",
    "    label_to_w = json.load(f)\n",
    "    \n",
    "train_Y = np.load(r'./work/cifar-100-train_Y.npy').reshape(-1, )\n",
    "test_Y = np.load(r'./work/cifar-100-test_Y.npy').reshape(-1, )\n",
    "# try:\n",
    "#     train_Y = np.load(r'./work/cifar-100-train_Y.npy').reshape(-1, )\n",
    "#     test_Y = np.load(r'./work/cifar-100-test_Y.npy').reshape(-1, )\n",
    "# except :\n",
    "#     train_Y = []\n",
    "#     for d in tqdm.tqdm(train_dataset):\n",
    "#         train_Y.append(label_to_truelabel[str(d[1])])\n",
    "#     train_Y = np.array(train_Y).reshape(-1,)\n",
    "#     np.save(r'./work/cifar-100-train_Y.npy', train_Y)\n",
    "#     test_Y = []\n",
    "#     for d in tqdm.tqdm(test_dataset):\n",
    "#         test_Y.append(label_to_truelabel[str(d[1])])\n",
    "#     test_Y = np.array(test_Y).reshape(-1,)\n",
    "#     np.save(r'./work/cifar-100-test_Y.npy', test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f1437f-be43-493a-a3ff-0702bcab91e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:11.998706Z",
     "iopub.status.busy": "2023-06-01T02:51:11.998474Z",
     "iopub.status.idle": "2023-06-01T02:51:12.006056Z",
     "shell.execute_reply": "2023-06-01T02:51:12.005210Z",
     "shell.execute_reply.started": "2023-06-01T02:51:11.998684Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# noise_ = np.where(np.random.binomial(1, noise_rate, train_Y.shape), 1,0)\n",
    "# train_Y_noised = train_Y.copy()\n",
    "# for i in range(len(noise_)):\n",
    "#     if noise_[i]>0:\n",
    "#         if np.mod(train_Y_noised[i]+1,5) == 0:\n",
    "#             train_Y_noised[i] -=4\n",
    "#         else:\n",
    "#             train_Y_noised[i] +=1\n",
    "noise_=np.where(np.random.binomial(n=1,p=noise_rate,size=train_Y.shape),np.random.randint(low=1,high=n_class-1,size=train_Y.shape),0)\n",
    "\n",
    "train_Y_noised = np.mod(noise_ + train_Y,n_class)\n",
    "train_Y_noised = np.array(train_Y_noised)\n",
    "Yt_list = [train_Y_noised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7033c6fe-cf34-4265-98d9-17889e8505a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:12.007479Z",
     "iopub.status.busy": "2023-06-01T02:51:12.007113Z",
     "iopub.status.idle": "2023-06-01T02:51:12.014575Z",
     "shell.execute_reply": "2023-06-01T02:51:12.013751Z",
     "shell.execute_reply.started": "2023-06-01T02:51:12.007455Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Orthogonal_loss17(paddle.nn.Layer):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss17, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.shape[0]\n",
    "        m = x.shape[1]\n",
    "\n",
    "        I = paddle.eye(m)\n",
    "        e = x - x.mean(axis=0, keepdim = True)\n",
    "        \n",
    "        cov = e.t() @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        # cov2 = paddle.abs(cov)\n",
    "        \n",
    "        select_i = paddle.argmax(cov2 - cov2 * I, axis = 1)\n",
    "        cov_m = (paddle.nn.functional.one_hot(select_i, m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / (n*m)\n",
    "        # result  = paddle.log(cov_m / cov_i + 1e-6)\n",
    "        return result\n",
    "        \n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_ortho = Orthogonal_loss17()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1181cf4c-3b23-4eda-b835-dd6b5f0d53c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:12.015764Z",
     "iopub.status.busy": "2023-06-01T02:51:12.015505Z",
     "iopub.status.idle": "2023-06-01T02:51:12.020092Z",
     "shell.execute_reply": "2023-06-01T02:51:12.019284Z",
     "shell.execute_reply.started": "2023-06-01T02:51:12.015742Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs_list = []\n",
    "pred_list = []\n",
    "Py_list = []\n",
    "PP_list = []\n",
    "Pm_list = []\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cadda1c7-c303-4aec-b4a3-0548124ebeb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:12.021536Z",
     "iopub.status.busy": "2023-06-01T02:51:12.021112Z",
     "iopub.status.idle": "2023-06-01T02:51:12.028758Z",
     "shell.execute_reply": "2023-06-01T02:51:12.028014Z",
     "shell.execute_reply.started": "2023-06-01T02:51:12.021512Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def cov_np(x, w=None):\n",
    "    if w is None:\n",
    "        n = x.shape[0]\n",
    "        cov = x.T @ x / n\n",
    "        e = np.mean(x, axis=0).reshape((-1, 1))\n",
    "        res = cov - e @ e.T\n",
    "    else:\n",
    "        w = w.reshape((-1, 1))\n",
    "        cov = (w * x).T @ x\n",
    "        e = np.sum(w * x, axis=0).reshape((-1, 1))\n",
    "        res = cov - e @ e.T\n",
    "\n",
    "    res= res ** 2\n",
    "    return res.sum() - np.trace(res)\n",
    "\n",
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    e = x - x.mean(axis = 0,keepdims = True)\n",
    "\n",
    "    cov = e.T @ e\n",
    "\n",
    "    sigma = (e ** 2).sum(axis = 0, keepdims = True)\n",
    "    r = cov / (sigma.T @ sigma) ** 0.5\n",
    "\n",
    "    r = r ** 2\n",
    "    r[np.isnan(r)] = 0.0\n",
    "\n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad07b23-3057-4c0e-8c65-da24b037a678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:12.032434Z",
     "iopub.status.busy": "2023-06-01T02:51:12.032065Z",
     "iopub.status.idle": "2023-06-01T02:51:12.045529Z",
     "shell.execute_reply": "2023-06-01T02:51:12.044741Z",
     "shell.execute_reply.started": "2023-06-01T02:51:12.032411Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    loss_eval = 0\n",
    "    acc_eval = 0\n",
    "    network_3.eval()\n",
    "    PP_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Py_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Pm_temp = np.zeros((test_samples,),dtype = np.float32)\n",
    "    Feature_l1_temp = np.zeros((test_samples,64),dtype = np.float32)\n",
    "    Feature_l2_temp = np.zeros((test_samples,128),dtype = np.float32)\n",
    "    Feature_l3_temp = np.zeros((test_samples,256),dtype = np.float32)\n",
    "    Feature_temp = np.zeros((test_samples,network_3.fc.weight.shape[0]),dtype = np.float32)\n",
    "\n",
    "    for batch_id, (ind, X_data) in enumerate(valid_loader()):\n",
    "        ind = ind.numpy()\n",
    "        Y_data = np.array(test_Y[ind]).astype(np.int64)\n",
    "        temp_X = paddle.to_tensor(X_data)\n",
    "        Y_GPU = paddle.to_tensor(Y_data)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "        first = network_3.maxpool(network_3.relu(network_3.bn1(network_3.conv1(temp_X))))\n",
    "        l1 = network_3.layer1(first)\n",
    "        l2 = network_3.layer2(l1)\n",
    "        l3 = network_3.layer3(l2)\n",
    "        l4 = network_3.layer4(l3)\n",
    "        feature_l1 = paddle.reshape(network_3.avgpool(l1), (l1.shape[0],-1))\n",
    "        feature_l2 = paddle.reshape(network_3.avgpool(l2), (l2.shape[0],-1))\n",
    "        feature_l3 = paddle.reshape(network_3.avgpool(l3), (l3.shape[0],-1))\n",
    "        feature = paddle.reshape(network_3.avgpool(l4), (l4.shape[0],-1))\n",
    "                \n",
    "        logits = network_3.fc(feature)\n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        \n",
    "        PP = paddle.sum(probs * probs, axis = -1)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pm = paddle.max(probs, axis = -1)\n",
    "        PP_temp[ind] = PP.numpy()\n",
    "        Py_temp[ind] = Py.numpy()\n",
    "        Pm_temp[ind] = Pm.numpy()\n",
    "        Feature_l1_temp[ind] = feature_l1.numpy()\n",
    "        Feature_l2_temp[ind] = feature_l2.numpy()\n",
    "        Feature_l3_temp[ind] = feature_l3.numpy()\n",
    "        Feature_temp[ind] = feature.numpy()\n",
    "        \n",
    "        loss = loss_fn(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "        acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "\n",
    "        loss_eval += loss.numpy()\n",
    "        acc_eval += acc.numpy()\n",
    "\n",
    "    loss_eval/=(batch_id+1)\n",
    "    acc_eval/=(batch_id+1)\n",
    "    lossb = relevant_hard_np(np.concatenate([Feature_l1_temp, Feature_l2_temp, Feature_l3_temp, Feature_temp], axis = -1))\n",
    "    # print('loss:%.4f, acc:%.4f, True_PP:%.4f, False_PP:%.4f'%(loss_eval, acc_eval, True_PP, False_PP))\n",
    "    return loss_eval, acc_eval, lossb, Feature_l1_temp, Feature_l2_temp, Feature_l3_temp, Feature_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d320cb6-79c5-4e7d-9119-3da9d57491d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T02:51:12.047110Z",
     "iopub.status.busy": "2023-06-01T02:51:12.046870Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0601 10:51:12.084864 17672 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0601 10:51:12.089440 17672 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train complete\r\n",
      "train loss:4.6811,train losso:0.0000, train acc:0.0358, train acc ori:0.1454,          eval loss:3.8370, eval acc:0.2565, lossb eval:3.373541e-01, time elapsed:22.2314\r\n",
      "epoch 0 train cleaned, 0 samples selected, 0 samples changed\r\n",
      "total remain noise:40030, max class noise:420\r\n",
      "epoch 1 train complete\r\n",
      "train loss:4.4714,train losso:0.0093, train acc:0.0740, train acc ori:0.3218,          eval loss:3.5773, eval acc:0.3679, lossb eval:2.343467e-01, time elapsed:43.8500\r\n",
      "f_prob_l4_val : 0.3625\r\n",
      "epoch 1 train cleaned, 5041 samples selected, 5041 samples changed\r\n",
      "total remain noise:38199, max class noise:414\r\n",
      "epoch 2 train complete\r\n",
      "train loss:4.1667,train losso:0.0214, train acc:0.1398, train acc ori:0.4182,          eval loss:3.0928, eval acc:0.4149, lossb eval:2.131326e-01, time elapsed:66.5389\r\n",
      "f_prob_l4_val : 0.3792\r\n",
      "epoch 2 train cleaned, 10102 samples selected, 10039 samples changed\r\n",
      "total remain noise:34692, max class noise:413\r\n",
      "epoch 3 train complete\r\n",
      "train loss:3.6984,train losso:0.0423, train acc:0.2288, train acc ori:0.4663,          eval loss:2.7675, eval acc:0.4544, lossb eval:1.935422e-01, time elapsed:89.3318\r\n",
      "f_prob_l4_val : 0.4019\r\n",
      "epoch 3 train cleaned, 12000 samples selected, 15048 samples changed\r\n",
      "total remain noise:31205, max class noise:408\r\n",
      "epoch 4 train complete\r\n",
      "train loss:3.1447,train losso:0.0670, train acc:0.3353, train acc ori:0.4953,          eval loss:2.5233, eval acc:0.4681, lossb eval:1.832655e-01, time elapsed:111.9263\r\n",
      "f_prob_l4_val : 0.3972\r\n",
      "epoch 4 train cleaned, 12393 samples selected, 20037 samples changed\r\n",
      "total remain noise:28618, max class noise:408\r\n",
      "epoch 5 train complete\r\n",
      "train loss:2.6427,train losso:0.0883, train acc:0.4337, train acc ori:0.5132,          eval loss:2.3853, eval acc:0.4837, lossb eval:1.718642e-01, time elapsed:134.4246\r\n",
      "f_prob_l4_val : 0.4211\r\n",
      "epoch 5 train cleaned, 13608 samples selected, 25014 samples changed\r\n",
      "total remain noise:26582, max class noise:403\r\n",
      "epoch 6 train complete\r\n",
      "train loss:2.2063,train losso:0.1036, train acc:0.5176, train acc ori:0.5241,          eval loss:2.2856, eval acc:0.4850, lossb eval:1.740471e-01, time elapsed:157.0267\r\n",
      "f_prob_l4_val : 0.4333\r\n",
      "epoch 6 train cleaned, 15209 samples selected, 30000 samples changed\r\n",
      "total remain noise:24830, max class noise:403\r\n",
      "epoch 7 train complete\r\n",
      "train loss:1.8769,train losso:0.1046, train acc:0.5782, train acc ori:0.5383,          eval loss:2.1524, eval acc:0.5012, lossb eval:1.747164e-01, time elapsed:179.3417\r\n",
      "f_prob_l4_val : 0.4565\r\n",
      "epoch 7 train cleaned, 16314 samples selected, 34696 samples changed\r\n",
      "total remain noise:23223, max class noise:396\r\n",
      "epoch 8 train complete\r\n",
      "train loss:1.6504,train losso:0.0945, train acc:0.6225, train acc ori:0.5549,          eval loss:2.0332, eval acc:0.5135, lossb eval:1.830242e-01, time elapsed:201.4419\r\n",
      "f_prob_l4_val : 0.4733\r\n",
      "epoch 8 train cleaned, 16374 samples selected, 38267 samples changed\r\n",
      "total remain noise:22008, max class noise:401\r\n",
      "epoch 9 train complete\r\n",
      "train loss:1.4661,train losso:0.0836, train acc:0.6566, train acc ori:0.5642,          eval loss:1.9217, eval acc:0.5389, lossb eval:1.887687e-01, time elapsed:223.6301\r\n",
      "f_prob_l4_val : 0.5046\r\n",
      "epoch 9 train cleaned, 14750 samples selected, 39000 samples changed\r\n",
      "total remain noise:21459, max class noise:407\r\n",
      "epoch 10 train complete\r\n",
      "train loss:1.3280,train losso:0.0723, train acc:0.6858, train acc ori:0.5740,          eval loss:1.9330, eval acc:0.5363, lossb eval:1.951368e-01, time elapsed:245.9422\r\n",
      "f_prob_l4_val : 0.5050\r\n",
      "epoch 10 train cleaned, 13274 samples selected, 39430 samples changed\r\n",
      "total remain noise:20905, max class noise:399\r\n",
      "epoch 11 train complete\r\n",
      "train loss:1.2272,train losso:0.0637, train acc:0.7113, train acc ori:0.5795,          eval loss:1.9340, eval acc:0.5478, lossb eval:1.929526e-01, time elapsed:268.7385\r\n",
      "f_prob_l4_val : 0.5157\r\n",
      "epoch 11 train cleaned, 11891 samples selected, 39622 samples changed\r\n",
      "total remain noise:20606, max class noise:404\r\n"
     ]
    }
   ],
   "source": [
    "network_3 = paddle.vision.models.resnet18(num_classes=n_class,pretrained=True)\n",
    "\n",
    "scheduel = paddle.optimizer.lr.ExponentialDecay(learning_rate = learning_rate, gamma = 0.95)\n",
    "\n",
    "opt = paddle.optimizer.Adam(learning_rate=learning_rate,\n",
    "epsilon=1e-06,\n",
    "parameters=network_3.parameters(),grad_clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0))\n",
    "# opt = paddle.optimizer.SGD(learning_rate = scheduel,\n",
    "#  parameters=network_3.parameters(),)\n",
    "\n",
    "\n",
    "# logits_list = []\n",
    "Pred_list = []\n",
    "Py_list = []\n",
    "PP_list = []\n",
    "Pm_list = []\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "noise_n_list = []\n",
    "select_n_list = []\n",
    "\n",
    "lossb_list = []\n",
    "\n",
    "score_list = [np.random.rand(samples,)]\n",
    "\n",
    "softmax = paddle.nn.Softmax(axis=0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_id in range(training_epochs):\n",
    "    network_3.train()\n",
    "    feature_num = network_3.fc.parameters()[0].shape[0]\n",
    "    loader = train_loader()\n",
    "\n",
    "    loss_train = 0\n",
    "    losso_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    PP_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pm_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pm_other_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp_other = np.zeros((samples,n_class),dtype=np.float32)\n",
    "\n",
    "    feature_l1_temp = np.zeros((samples, 64), dtype = np.float32)\n",
    "    feature_l2_temp = np.zeros((samples, 128), dtype = np.float32)\n",
    "    feature_l3_temp = np.zeros((samples, 256), dtype = np.float32)\n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    " \n",
    "    if len(Py_list)>0:\n",
    "        PP_mean = PP_list[-1]\n",
    "        Py_mean = Py_list[-1]\n",
    "    else:\n",
    "        PP_mean = np.zeros((samples,))\n",
    "        Py_mean = np.zeros((samples,))\n",
    "\n",
    "    thres = np.sort(score_list[-1])[int(samples*0.8)]\n",
    "    r_mask = score_list[-1] >= thres\n",
    "    side_mask = np.logical_and(r_mask,np.random.rand(samples,) < 0.2) \n",
    "    \n",
    "    Y_onehot = np.eye(n_class)[Yt_list[-1]]\n",
    "    Y_onehot_0 = np.eye(n_class)[Yt_list[0]]\n",
    "    \n",
    "    for batch_id, (ind, X_data) in enumerate(loader):\n",
    "        ind = ind.numpy()\n",
    "\n",
    "        Y_data = np.array(Yt_list[-1][ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[ind]).astype(np.int64)\n",
    "        temp_X = paddle.to_tensor(X_data)\n",
    "        Y_GPU = paddle.to_tensor(Y_data)\n",
    "        Y_GPU_ori = paddle.to_tensor(Y_data_ori)\n",
    "        y_onehot = paddle.nn.functional.one_hot(paddle.reshape(Y_GPU,(-1,)),num_classes=n_class)\n",
    "\n",
    "        first = network_3.maxpool(network_3.relu(network_3.bn1(network_3.conv1(temp_X))))\n",
    "        l1 = network_3.layer1(first)\n",
    "        l2 = network_3.layer2(l1)\n",
    "        l3 = network_3.layer3(l2)\n",
    "        l4 = network_3.layer4(l3)\n",
    "        feature_l1 = paddle.reshape(network_3.avgpool(l1), (l1.shape[0],-1))\n",
    "        feature_l2 = paddle.reshape(network_3.avgpool(l2), (l2.shape[0],-1))\n",
    "        feature_l3 = paddle.reshape(network_3.avgpool(l3), (l3.shape[0],-1))\n",
    "        feature = paddle.reshape(network_3.avgpool(l4), (l4.shape[0],-1))\n",
    "\n",
    "        logits = network_3.fc(feature)\n",
    "\n",
    "        probs = paddle.nn.functional.softmax(logits)\n",
    "        PP = paddle.sum(probs * probs, axis = -1)\n",
    "        Py = paddle.sum(y_onehot * probs, axis = -1)\n",
    "        Pm = paddle.max(probs, axis = -1)\n",
    "        Pm_other = paddle.max(probs - probs * y_onehot, axis = -1)\n",
    "        Pred = paddle.argmax(probs,axis=-1)\n",
    "        logits_other = logits - y_onehot*1e10\n",
    "        Pred_other = paddle.argmax(logits_other,axis=-1)\n",
    "\n",
    "        alpha = 20\n",
    "        # alpha = 1e-1\n",
    "\n",
    "        if epoch_id < t_w:\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "            loss_o = paddle.to_tensor(np.array([0.]))\n",
    "            loss_g = loss\n",
    "        else:\n",
    "            boundary_mask_ = paddle.to_tensor(side_mask[ind])\n",
    "            label_rand = paddle.randint(low=0, high = n_class, shape=Y_GPU.shape)            \n",
    "            Y_GPU = paddle.where(boundary_mask_, label_rand, Y_GPU)\n",
    "            loss = loss_fn(logits,paddle.reshape(Y_GPU,(-1,1)))\n",
    "            loss_o = loss_ortho(paddle.concat([feature_l1, feature_l2, feature_l3, feature], axis = -1))\n",
    "            loss_g = loss + loss_o\n",
    "\n",
    "\n",
    "            \n",
    "        PP_temp[ind] = PP.numpy()\n",
    "        Py_temp[ind] = Py.numpy()\n",
    "        Pm_temp[ind] = Pm.numpy()\n",
    "        Pm_other_temp[ind] = Pm_other.numpy()\n",
    "        Pred_temp[ind] = Pred.numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.numpy()\n",
    "        Logits_temp[ind] = logits.numpy()\n",
    "        Logits_temp_other[ind] = logits_other.numpy()\n",
    "        feature_l1_temp[ind] = feature_l1.numpy()\n",
    "        feature_l2_temp[ind] = feature_l2.numpy()\n",
    "        feature_l3_temp[ind] = feature_l3.numpy()\n",
    "        feature_temp[ind] = feature.numpy()\n",
    "                \n",
    "        acc = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU,(-1,1)))\n",
    "        acc_ori = paddle.metric.accuracy(logits, paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "        loss_ori = loss_fn(logits,paddle.reshape(Y_GPU_ori,(-1,1)))\n",
    "\n",
    "        loss_train += loss.numpy()\n",
    "        losso_train += loss_o.numpy()\n",
    "        acc_train += acc.numpy()\n",
    "        acc_train_ori += acc_ori.numpy()\n",
    "        loss_train_ori += loss_ori.numpy()\n",
    "        \n",
    "        loss_g.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "    scheduel.step()\n",
    "\n",
    "        \n",
    "    loss_train/=(batch_id+1)\n",
    "    losso_train/=(batch_id+1)\n",
    "    acc_train/=(batch_id+1)\n",
    "    acc_train_ori/=(batch_id+1)\n",
    "    loss_train_ori/=(batch_id+1)\n",
    "\n",
    "    print('epoch %d train complete'%epoch_id)\n",
    "    loss_eval, acc_eval, lossb_eval, feature_l1_val, feature_l2_val, feature_l3_val, feature_val = validation()\n",
    "\n",
    "    loss_list.append(loss_eval)\n",
    "    acc_list.append(acc_eval)\n",
    "    Pred_list.append(Pred_temp)\n",
    "    PP_list.append(PP_temp)\n",
    "    Py_list.append(Py_temp)\n",
    "    Pm_list.append(Pm_temp)\n",
    "    \n",
    "    lossb_list.append(lossb_eval)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        'train loss:%.4f,train losso:%.4f, train acc:%.4f, train acc ori:%.4f,\\\n",
    "          eval loss:%.4f, eval acc:%.4f, lossb eval:%e, time elapsed:%.4f'\n",
    "    %(loss_train,losso_train, acc_train, acc_train_ori,\n",
    "     loss_eval, acc_eval, lossb_eval, end_time - start_time))  \n",
    "    \n",
    "    if epoch_id < t_w:\n",
    "        temp_Yt_noised = Yt_list[-1]\n",
    "        select = np.zeros((samples,),dtype = np.bool)\n",
    "        score = np.random.rand(samples,)\n",
    "    else:\n",
    "        feature_gpu = paddle.to_tensor(\n",
    "            np.concatenate([feature_l1_temp, feature_l2_temp, feature_l3_temp, feature_temp, np.ones((samples,1))],\n",
    "         axis = -1))\n",
    "        Y_onehot_gpu = paddle.to_tensor(Y_onehot,dtype = paddle.float32)\n",
    "        feature_gpu_val = paddle.to_tensor(\n",
    "            np.concatenate([feature_l1_val, feature_l2_val, feature_l3_val, feature_val, np.ones((test_samples,1))],\n",
    "         axis = -1))\n",
    "\n",
    "        n_g = 5\n",
    "        f_prob = np.zeros((samples, n_class))\n",
    "        f_prob_val = np.zeros((test_samples, n_class))\n",
    "        for i_w in range(n_g):\n",
    "            m_v = paddle.to_tensor(np.where(np.random.rand(samples,) <= 0.8)[0])\n",
    "            m_h = paddle.to_tensor(np.where(np.random.rand(feature_gpu.shape[1],) <= 0.5)[0])\n",
    "            feature_gpu_mh = paddle.gather(feature_gpu, axis = 1, index = m_h)\n",
    "\n",
    "            W = paddle.linalg.inv(feature_gpu_mh[m_v].T @ feature_gpu_mh[m_v]) @ feature_gpu_mh[m_v].T @ Y_onehot_gpu[m_v]\n",
    "\n",
    "            f_prob = f_prob + (feature_gpu_mh @ W).numpy()\n",
    "            f_prob_val = f_prob_val + (paddle.gather(feature_gpu_val, axis = 1, index = m_h) @ W).numpy()\n",
    "        f_prob = f_prob / n_g\n",
    "        f_prob_val = f_prob_val / n_g\n",
    "\n",
    "        f_pred = np.argmax(f_prob, axis = -1)\n",
    "        \n",
    "        \n",
    "        print('f_prob_l4_val : %.4f'%((np.argmax(f_prob_val, axis = -1) == test_Y).mean()))\n",
    "\n",
    "        score = np.argsort(np.argsort(np.sum((Y_onehot_0 - f_prob)**2, axis = -1)))\n",
    "\n",
    "        revise_predict = Pred_temp\n",
    "\n",
    "        select = np.zeros((samples,),dtype= bool)\n",
    "\n",
    "\n",
    "        r_ = min(0.1 + 0.1 * (epoch_id - t_w), 0.8)\n",
    "\n",
    "        for j_ in range(n_class):\n",
    "            class_mask = (Yt_list[0] == j_).ravel()\n",
    "            class_n = class_mask.sum()\n",
    "            if class_n > 0:\n",
    "                class_thres = np.sort(score[class_mask])[int(class_n * (1 - r_))]\n",
    "            else:\n",
    "                class_thres = 1\n",
    "            select[np.logical_and(class_mask, score >= class_thres)] = True\n",
    "            \n",
    "        temp_Yt_noised = np.where(select.ravel(), revise_predict.ravel(), Yt_list[0].ravel()).astype(int)\n",
    "\n",
    "    is_noise = temp_Yt_noised != train_Y\n",
    "    max_noised_class = -999\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = train_Y == j_\n",
    "        noise_n = np.logical_and(class_mask, is_noise).sum()\n",
    "        if noise_n > max_noised_class:\n",
    "            max_noised_class = noise_n\n",
    "    Yt_list.append(temp_Yt_noised)\n",
    "    print('epoch %d train cleaned, %d samples selected, %d samples changed'%(\n",
    "        epoch_id,np.sum(Yt_list[-1]!=Yt_list[-2]), np.sum(Yt_list[-1]!=Yt_list[0])))\n",
    "    Yt_remain_noise = np.sum(is_noise)\n",
    "    print('total remain noise:%.4d, max class noise:%d'%(Yt_remain_noise, max_noised_class))\n",
    "    noise_n_list.append(Yt_remain_noise)\n",
    "    select_n_list.append(np.sum(select))\n",
    "    score_list.append(score)\n",
    "\n",
    "\n",
    "    # with open(r'./work_mechanism/cifar100/acc_eval_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%acc_eval)\n",
    "    # with open(r'./work_mechanism/cifar100/loss_eval_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%loss_eval)\n",
    "    # with open(r'./work_mechanism/cifar100/acc_train_ori_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%acc_train_ori)\n",
    "    # with open(r'./work_mechanism/cifar100/loss_train_ori_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%loss_train_ori)\n",
    "    # with open(r'./work_mechanism/cifar100/acc_train_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%acc_train)\n",
    "    # with open(r'./work_mechanism/cifar100/loss_train_1226.txt','a') as f:\n",
    "    #     f.write('%.8f\\n'%loss_train)\n",
    "\n",
    "    # loss_unweighted = relevant_hard_np(feature_temp)\n",
    "    \n",
    "    # print('unweighted loss %e'%(loss_unweighted))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af8abf-25be-4226-8b3a-b72b79674bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9dbb3-61bf-489c-9e81-7727ec598bf3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cifar100, 80 uniform flip : 57.5\n",
    "#cifar100, 60 uniform flip : 68.9\n",
    "#cifar100, 40 uniform flip : 72.5\n",
    "#cifar100, 20 uniform flip : 74.4\n",
    "\n",
    "#cifar100, 40 pair flip : 72.8\n",
    "#cifar100, 30 pair flip : 73.9\n",
    "#cifar100, 20 pair flip : 75.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ba287-90b1-4c0e-ba65-e02d1fdafd3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8e148-be18-4805-bde0-33f354fd9c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paddle.save(network_3.state_dict(), r'./work/network_3.pdparams')\n",
    "# paddle.save(opt.state_dict(), r'./work/opt.pdopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a1682-cd67-4335-a18a-5fb1bc626059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save('./work/logits_list.npy',np.concatenate(logits_list, axis = 0))\n",
    "# np.save('./work/logits_noise.npy',np.concatenate(logits_noise, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccd442-f4a5-4173-9998-4ee28f18b081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
